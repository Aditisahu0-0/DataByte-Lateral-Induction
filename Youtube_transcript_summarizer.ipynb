{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 951,
   "id": "808fdb1f-b0e6-49e9-93cf-7c272f251ab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "import numpy as np \n",
    "import re\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk import pos_tag\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 953,
   "id": "a45202e8-4329-4910-8450-d20a9b16ebca",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp= spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 803,
   "id": "842fa160-86a0-47da-b748-1cff07939a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript(video_id):\n",
    "    try:\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)  #transcript stores in dictionaries inside list , where key is 'text'\n",
    "        transcript_text = \" \".join([item['text'] for item in transcript]) #to get values of all the dictionaries and combine them to form a string text.\n",
    "        return transcript_text\n",
    "    except Exception as e:\n",
    "        print(\"Error fetching transcript:\",e)\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 769,
   "id": "77568d3d-4366-48ee-96e8-52fd441ab303",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "   \n",
    "    text = re.sub(r'\\[.*?\\]', '', text)  # Remove non- verbal sounds like [music]\n",
    "    \n",
    "    text = re.sub(r'\\s+', ' ', text)  # Remove extra spaces\n",
    "    \n",
    "    text = text.strip()\n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 771,
   "id": "5cad7d72-4dab-43c1-80fb-2bc38017f6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = clean_text(text)\n",
    "    text = text.lower()\n",
    "    doc=nlp(text)\n",
    "    \n",
    "    stemmer=PorterStemmer()\n",
    "    stemmed_text = [ stemmer.stem(token.text) for token in doc ] #nltk\n",
    "\n",
    "    lemmatized_text = [token.lemma_ for token in doc]  #spacy\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_text=[]\n",
    "    \n",
    "    for word in lemmatized_text:\n",
    "        if word not in stop_words:\n",
    "            filtered_text.append(word)\n",
    "\n",
    "    text=\" \".join(filtered_text)\n",
    "    text = re.sub(r'[^\\w\\s]+', '', text)# Remove punctuation and music symbol or any symbol\n",
    "    \n",
    "    return text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 971,
   "id": "1be9d686-1d11-4b4a-9c89-6634a8ad3978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizer1(text, n=3):\n",
    "    doc=nlp(text)\n",
    "    \n",
    "    processed_text = preprocess_text(text).split()\n",
    "    \n",
    "    pos_tags= pos_tag(processed_text) #stores as list of tuples \n",
    "    \n",
    "    word_freq = {}\n",
    "    for word,pos in pos_tags:\n",
    "            if pos==\"NN\" or pos==\"VBP\":\n",
    "                word_freq[word] = word_freq.get(word,0)+ 1 #default value is 0\n",
    "\n",
    "    sentence_score={}\n",
    "    for sentence in doc.sents:\n",
    "        sentence_str = clean_text(sentence.text)\n",
    "        for token in sentence:\n",
    "            if token.text in word_freq:\n",
    "                sentence_score[sentence_str] = sentence_score.get(sentence_str,0)+ word_freq[token.text]\n",
    "                \n",
    "    summarized_text= sorted(sentence_score, key=sentence_score.get, reverse=True )[:n]\n",
    "    \n",
    "    return \". \".join(s.capitalize() for s in summarized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 973,
   "id": "b64bf23c-9641-40f1-ad3e-00086f0cf356",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarizer2(text, n=3):\n",
    "    doc = nlp(text)\n",
    "    sentences= []\n",
    "    for sentence in doc.sents:\n",
    "        sentences.append(sentence.text)\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words=\"english\")\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(sentences)\n",
    "    \n",
    "    sentence_score = np.array(tfidf_matrix.sum(axis=1)).flatten()\n",
    "    \n",
    "    top_sent_indices= sentence_score.argsort()[-n:][::-1]\n",
    "    \n",
    "    top_sents= [sentences[i] for i in top_sent_indices]\n",
    "    \n",
    "    return \". \".join(s.capitalize() for s in top_sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 975,
   "id": "72dccb7b-075b-47cc-aa7d-71bf9c407a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter a youtube video url: https://www.youtube.com/watch?v=hOCDJyZ6quA\n",
      "Enter no. of lines of summary needed: 3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SUMMARY OF THE TRANSCRIPT: OPTION 1\n",
      "Okay. I'm going to link a good article here so in the sequential model you add layers one by one as a sequence you see but in a functional model you create your input then you create a hidden layer let's say and supply input as a function argument then you create hidden one then you supply that into hidden two's argument and so on and then you create model using inputs and outputs now this allows you to create a model which might have multiple inputs multiple outputs like something like rest net you know you can also share network layers with other other models so there are there are some differences you read the article you will get an idea so here i'm going to build a functional model. And when you use the about encoder it returns a dictionary out of which you need to use pulled out pull output is basically the encoding for the entire sentence again if you want to know what other elements are there in the dictionary you need to watch my previous video it's sort of like a prerequisite all right now see when i run this it is generating for this sentence this is my embedding vector the size is 768 for this second sentence this is my embedding vector so we have achieved the major goal here which is generating the vector you know vector using the bert and i just give you a simple function but in reality we will be using tensorflow layers\n",
      "\n",
      "SUMMARY OF THE TRANSCRIPT: OPTION 2\n",
      "So this tutorial provided you a very like a simple explanation of how you can do text classification using bert you can use bert for variety of other problems as well just such as movie review classification or name entity recognization and by the way i have an exercise for you and the exercise is actually very simple you have to just do copy paste so go to google red tensorflow tutorial and in that go to text tutorial and look at classified text with bert so what you need to do is you need to just run this code on your computer so just copy paste these these lines you know step by step in your notebook and just run it and try to understand it this tutorial is similar to what we did but the data set is much bigger they are using tensorflow data set api so in terms of api also they are using little different we use pandas and they are also using some caching the model is also little different. Okay when you run it it's gonna take some time because it is downloading the bert model you know it's somewhat around 300 megabytes so based on your internet speed it might take some time but essentially you are downloading a train model which is trained on all the wikipedia and book corpus so now in our task we'll be just directly using that train model to generate the embedding vectors after model is downloaded i am going to define a simple function that takes couple of sentences as an input and returns me an embedding vector so basically the way i'll use this function is okay supply an array of sentences and any sentence. And when you use the about encoder it returns a dictionary out of which you need to use pulled out pull output is basically the encoding for the entire sentence again if you want to know what other elements are there in the dictionary you need to watch my previous video it's sort of like a prerequisite all right now see when i run this it is generating for this sentence this is my embedding vector the size is 768 for this second sentence this is my embedding vector so we have achieved the major goal here which is generating the vector you know vector using the bert and i just give you a simple function but in reality we will be using tensorflow layers\n"
     ]
    }
   ],
   "source": [
    "video_url = input(\"Enter a youtube video url:\")\n",
    "n= int(input(\"Enter no. of lines of summary needed:\"))\n",
    "\n",
    "if \"v=\" in video_url:\n",
    "    video_id = video_url.split(\"v=\")[-1]\n",
    "else:\n",
    "    video_id = video_url.split(\"be/\")[-1]     #returns a list\n",
    "    \n",
    "text = get_transcript(video_id)\n",
    "\n",
    "print(\"\\nSUMMARY OF THE TRANSCRIPT: OPTION 1\")\n",
    "summarized_text1= summarizer1(text, n)\n",
    "print(summarized_text1)\n",
    "\n",
    "print(\"\\nSUMMARY OF THE TRANSCRIPT: OPTION 2\")\n",
    "summarized_text2= summarizer2(text, n)\n",
    "print(summarized_text2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45701e50-c416-4ddf-9e77-9c2ae94d8769",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
